#+TITLE: Machine Learning related notes from books, courses, videos etc.

* Python ML, Learning by Example
+ 9781800209718
+ [[https://www.packtpub.com/product/python-machine-learning-by-example-third-edition/9781800209718][link]]

** Chapter 1
*** 3 types of ML
**** Unsupervised
+ find structure in unlabelled, often noisy data
+ often applied in:
  - fraud detection systems
  - grouping customers based on behaviour for marketing campaigns
+ related concepts:
  - data visualisation -> makes data more digestible
  - dimensionality reduction -> distils only relevant information
  - clustering -> label/classify data

***** Semi-supervised
+ subdivision of unsupervised learning
+ large portion of available data is unlabelled, but isn't
+ uses unlabelled data for training and labelled for validation
+ applied when:
  - dataset is incomplete
  - it is too expensive/inefficient to classify entire dataset

**** Supervised
+ data comes pre-labelled and /often/ clean
+ goal is to find a general rule that maps *input* to *output*
+ labels are often produced through automated event-loggers
+ often applied in:
  - face & speech recognition
  - *sales forecasting*
+ can be subdivided into *regression* and *classification*

***** Regression
+ trains on and predicts continuous-valued responses
+ e.g. housing prices

***** Classification
+ attempts to find the appropriate class label
+ e.g. sentiment analysis

**** Reinforcement
+ learning data provides feedback
+ enables system to dynamically adapt to environmental stimuli
+ system evaluates performance based on feedback responses
+ often applied in:
  - industrial automation
  - self-driving cars
  - chess engines

*** 4 types of ML Algorithms
**** Logic-based
+ learn basic rules specified by human experts
+ use said rules to (try to) reason using formal logic, background knowledge and hypotheses

**** Statistical Learning
+ attempt to find a function to formalize the relationships between variables

**** Artificial Neural Networks (ANNs)
+ consist of interconnected neurons that attempt to imitate brains
+ attempt to model complex relationships between *input* and *output* values and to capture *patterns* in data

**** Genetic Algorithms (GAs)
+ mimic the biological process of evolution and try to find the optimal solutions
+ often use methods like /crossover/ and /mutation/, /epochs/ and /generations/

*** Why are GPUs good at ML?
+ GPUs enabled the breakthrough in deep learning of 2006
  - deep learning -> ANNs with many layers
+ GPUs are great at handling *specialised* tasks, rather than general tasks (CPU)

*** Supervised Learning Metaphor
+ think of it as studying for an exam
+ past papers = training data
+ exam = test data
+ mock exam = validation data
  - used to evaluate how a model will generalise to independent or unseen datasets in a simulated setting

*** Overfitting
+ overfitting = memorisation
+ model performance is a function where bias and variance are *minimised*
+ low bias and high variance indicate overfitting; good average prediction to true value ratio within the training data but bad generalisation to test data
+ [[./images/overfitting.png][overfitting]]
+ in the above example the regression curve attempts to flawlessly accommodate all observed samples
+ avoid/combat overfitting one or a combination of the following methods

**** Cross-validation
+ method of evaluating a model's performance
  - tests the model's hyper-parameters mostly
+ exhaustive vs non-exhaustive
+ k-fold is non-exhaustive since it doesn't try out all possible partitions of the given data

***** K-Fold
1. Shuffle dataset randomly
2. Split dataset in /k/ groups
3. For each unique group:
   1. Take the group as a hold out or test dataset
   2. Take the remaining groups as a training dataset
   3. Fit a model on the training set and evaluate it on the test set
   4. Retain the evaluation score and *discard the model*
4. Summarise the skill of the model using the sample of model evaluation scores

**** Data Regularisation
+ the principle of Occam's razor applies in ML
+ simpler models are usually better
+ they take less time to train and less resources to run
+ they also overfit data less easily
+ generalising the rules learnt by the model can help it capture a wider range of data in the real world

**** Feature selection & Dimensionality reduction
+ number of columns = dimensionality
+ fitting high-dimensional data can be computationally expensive
+ it is also prone to overfitting due to initial high complexity
+ this method aims to remove unnecessary features that add no value

*** Underfitting
+ model performs badly on both training and testing data
+ it has failed to capture the underlying *trend* of the data
+ happens when:
  - insufficient data is given to the model
  - wrong type of model is applied

*** Bias & Variance
**** Bias
+ the error stemming from incorrect assumptions in the learning algorithm
+ high bias results in underfitting

**** Variance
+ measures how sensitive a model's predictions are to variations in the datasets

**** Bias-variance trade-off
+ bias and variance should ideally be minimised in order to maximise results
+ in practice, the two are correlated in the sense that decreasing one, increases the other

**** Example Scenario
+ you're asked to build a model that predicts the probability of a presidential candidate being elected
+ the available data is phone poll data in the form of zip codes
+ by choosing a random zip code and using that estimate a 61% probability you introduce *high* bias (people in a geographic area tend to share similar demographics)
+ the same method introduces *low* variance

*** Mean Squared Error (MSE)
+ model performance can be scored using various methods
+ one of them is MSE
+ [[./images/mse.png][MSE]]

*** CRISP-DM
+ Cross-Industry Standard Process for Data Mining
+ created in 1996
+ machine learning / data science inherits its phases and general framework
  1. Business understanding
  2. Data understanding AKA exploration phase
  3. Data preparation AKA preprocessing phase
  4. Modeling
  5. Evaluation
  6. Deployment

*** Label Encoding
+ the process of changing given data types (labels) to numbers

**** One-hot Encoding
+ assigns a True value to the correct label and false to everything else
+ ends up producing a sparse matrix instead of a vector
+ size of matrix is k^2 given that the original number of labels is k

*** Polynomial transformation
+ creates more features out of the existing ones
+ in a dataset of 2 features, a and b, a third one could be a*b or a/b
+ in the case of the ratio a small constant should be added to both the divisor and dividend to avoid dividing by zero

***
